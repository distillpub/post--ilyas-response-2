<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>
<style>
body {
  margin: 40px;
}

li {
margin-bottom:0 !important
}

ul {
margin-bottom:0 !important
}

.definition {
  margin: 10px 50px 10px 50px;
  border: 0.5px solid rgb(225,225,225);
  padding: 10px;
  border-radius: 5px;
  box-shadow: 0px 0px 8px rgba(0,0,0,0.04);
}

.wrapper {
  display: grid;
  grid-template-columns: 100px 100px 100px;
  grid-gap: 10px;
  background-color: #fff;
  color: #444;
  width: 50%;
  margin: 0 auto;
}

.box {
  background-color: #444;
  color: #fff;
  border-radius: 5px;
  padding: 20px;
  font-size: 150%;
}


.left {
  text-align: left;
  line-height: 20px
}

.right {
  text-align: left;
  line-height: 20px
}

.grey {
  color:rgb(140,140,140);
}


.mono {
  font-family: monospace;
}

</style>


<d-front-matter>
  <script type="text/json">{
  "title": "Robust Feature Leakage",
  "description": "An example project using webpack and svelte-loader and ejs to inline SVGs",
  "password": "bugs",
  "authors": [
    {
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>

<d-article>

<p>
Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> show that a model trained on adversarial examples has effectiveness on clean data. This is a surprising result. The mechanism behind this, <d-cite key="ilyas2019adversarial"></d-cite> suggests, is that this transfer is driven primarily by an imprinting of non-robust cues in the process of the attack. An alternative mechanism for this, however, is the possibility of adversarial examples imprinting robust cues in the image. The extent to which robust cues may drive the accuracies reported in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> is a problem we refer to as <i>robust feature leakage</i>. 
</p>

<p>
In this manuscript we establish that at least 23.5% (out of 88%) of the accuracy can be explained by robust features in $D_\text{rand}$. This simple lower bound, established by a linear model, can very likely be improved. We find, however, no evidence of leakage in $D_\text{det}$.
</p>

<h3>Lower Bounding Leakage</h3>
<p>
Our technique for quantifying leakage consisting of two steps:
  </p>  
  <div style="margin: 0px 10px 0px 10px">
    <ol type="1">
      <li>First, we construct features $f_i(x) = w_i^Tx$ that are provably robust, in a sense we will soon specify.</li>
      <li>Next, we train a linear classifier <d-footnote>as per <d-cite key="ilyas2019adversarial"></d-cite>, Equation 3</d-footnote> on the datasets $\hat{\mathcal{D}}_{\text{det}}$ and $\hat{\mathcal{D}}_{\text{rand}}$ (Defined <d-cite key="ilyas2019adversarial"></d-cite>, Table 1) on these robust features <i>only</i>.
    </ol>
  </div>

  <p>
  We consider accuracy of the linear classifier above as the accuracy explained by robust features only. We propose two specifications for what constitutes a <i>robust feature</i>:

    <div class="definition">
      <span style="font-variant: small-caps;"><b> Specification 1</b></span> &nbsp <i>Multiclass $\gamma$-robust Usefulness</i><br>
      A feature that is $\gamma$-robustly useful <d-cite key="ilyas2019adversarial"> </d-cite> for at least one of the classes, with $\gamma = 0$, and the set of valid perturbations equal to an $L_2$ norm ball with radius 0.25.
    </div>

    <div class="definition">
      <span style="font-variant: small-caps;"><b> Specification 2</b></span> &nbsp <o><i>Robustness Radius</i><br>
      A feature that comes from a robust model for which at least 80% of points in the test set have predictions that remain static in a neighborhood of radius 0.25 on the $L_2$ norm ball.
    </div>

  </p>

  <p>
  We find features that satisfy <i>both</i> specifications by using the 10 linear features of a robust linear model trained on CIFAR-10. Because the features are linear, the above two conditions can be certified analytically. We leave the reader to inspect the weights corresponding to the features manually:
  </p>
  <figure id="robust" style="grid-column: screen;">
  </figure>

  <p>
  Training a linear model on the above robust features on $\hat{\mathcal{D}}_{\text{rand}}$ incurs an accuracy of <b>23.5%</b> (out of 88%), while training on $\hat{\mathcal{D}}_{\text{det}}$ incurs an accuracy of <b>6.81%</b> (out of 44%).
  </p>
  <p>
  The contrasting results suggest that the the two experiements should be interpreted differently. The transfer results of $\hat{\mathcal{D}}_{\text{rand}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> should approached with caution: A non-trivial portion of the accuracy can be attributed to robust features. This bound is too, a lower bound based on linear features alone. This number may be higher if we used nonlinear robust features, taken from, say, a robust deep neural network.
  </p>
  <p>
  The results of $\hat{\mathcal{D}}_{\text{det}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> however, are on stronger footing. We find no evidence of feature leakage (in fact, we find negative leakage &#8212; an influx!). We thus conclude that it is very plausible the majority of the accuracy is driven by non-robust features, exactly the thesis of  <d-cite key="ilyas2019adversarial"></d-cite>. 
  </p>
<!--   <p>
  The contrasting results, thus, point to methodological advise &#8212; when disentangling two cues in an image, we believe safer is better to drive correlations negative than it is to 0.
  </p>  -->

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
