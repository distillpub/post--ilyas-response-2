<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>
<style>
body {
  margin: 40px;
}

li {
margin-bottom:0 !important
}

ul {
margin-bottom:0 !important
}

.definition {
  margin: 10px 50px 10px 50px;
  border: 0.5px solid rgb(225,225,225);
  padding: 10px;
  border-radius: 5px;
  box-shadow: 0px 0px 8px rgba(0,0,0,0.04);
}

.wrapper {
  display: grid;
  grid-template-columns: 100px 100px 100px;
  grid-gap: 10px;
  background-color: #fff;
  color: #444;
  width: 50%;
  margin: 0 auto;
}

.box {
  background-color: #444;
  color: #fff;
  border-radius: 5px;
  padding: 20px;
  font-size: 150%;
}


.left {
  text-align: left;
  line-height: 20px
}

.right {
  text-align: left;
  line-height: 20px
}

.grey {
  color:rgb(140,140,140);
}


.mono {
  font-family: monospace;
}

</style>


<d-front-matter>
  <script type="text/json">{
  "title": "Robust Feature Leakage",
  "description": "An example project using webpack and svelte-loader and ejs to inline SVGs",
  "authors": [
    {
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>

<d-article>


<p>
Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> report a surprising result: a model trained on adversarial examples is effective on clean data. They suggest this transfer is driven by adverserial examples containing geuinely useful non-robust cues. But an alternate mechanism for the transfer could be a kind of "robust feature leakage" where the model picks up on faint robust cues in the attacks.
</p>

<!-- 
<p>
Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> show that a model trained on adversarial examples is effective on clean data, a highly surprising result. The primary driver behind this, <d-cite key="ilyas2019adversarial"></d-cite> suggests, is the non-robust cues overlaid on the dataset by the adversery. If adverserial examples contain robust cues too, however, (light as they may be) this presents an alternative mechanism for the transfer. 
</p>
 -->


<p>
We show that at least 23.5% (out of 88%) of the accuracy can be explained by robust features in $D_\text{rand}$. This is a weak lower bound, established by a linear model, and does not perclude the possibility of further leakage. On the other hand, we find no evidence of leakage in $D_\text{det}$. 
</p>

<h3>Lower Bounding Leakage</h3>
<p>
Our technique for quantifying leakage consisting of two steps:
  </p>  
  <div style="margin: 0px 10px 0px 10px">
    <ol type="1">
      <li>First, we construct features $f_i(x) = w_i^Tx$ that are provably robust, in a sense we will soon specify.</li>
      <li>Next, we train a linear classifier <d-footnote>as per <d-cite key="ilyas2019adversarial"></d-cite>, Equation 3</d-footnote> on the datasets $\hat{\mathcal{D}}_{\text{det}}$ and $\hat{\mathcal{D}}_{\text{rand}}$ (Defined <d-cite key="ilyas2019adversarial"></d-cite>, Table 1) on these robust features <i>only</i>.
    </ol>
  </div>

  <p>
  Since Ilyas et al. <d-cite key="ilyas2019adversarial"></d-cite> only specify robustness in the two class case, we propose two possible specifications for what constitutes a <i>robust feature</i> in the multiclass setting:

    <div class="definition">
      <span style="font-variant: small-caps;"><b> Specification 1</b></span><br>For at least one of the classes, the feature is $\gamma$-robustly useful <d-cite key="ilyas2019adversarial"> </d-cite> with $\gamma = 0$, and the set of valid perturbations equal to an $L_2$ norm ball with radius 0.25.
    </div>

    <div class="definition">
      <span style="font-variant: small-caps;"><b> Specification 2</b></span> <br>
      The feature comes from a robust model for which at least 80% of points in the test set have predictions that remain static in a neighborhood of radius 0.25 on the $L_2$ norm ball.
    </div>

  </p>

  <p>
  We find features that satisfy <i>both</i> specifications by using the 10 linear features of a robust linear model trained on CIFAR-10. Because the features are linear, the above two conditions can be certified analytically. We leave the reader to inspect the weights corresponding to the features manually:
  </p>
  <figure id="robust" style="grid-column: screen;">
  </figure>

  <p>
  Training a linear model on the above robust features on $\hat{\mathcal{D}}_{\text{rand}}$ and testing on the CIFAR test set incurs an accuracy of <b>23.5%</b> (out of 88%). Doing the same on $\hat{\mathcal{D}}_{\text{det}}$ incurs an accuracy of <b>6.81%</b> (out of 44%).
  </p>
  <p>
  The contrasting results suggest that the the two experiements should be interpreted differently. The transfer results of $\hat{\mathcal{D}}_{\text{rand}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> should approached with caution: A non-trivial portion of the accuracy can be attributed to robust features. Note that this bound is weak: this bound could be possibly be improved if we used nonlinear features, e.g. from a robust deep neural network.
  </p>
  <p>
  The results of $\hat{\mathcal{D}}_{\text{det}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> however, are on stronger footing. We find no evidence of feature leakage (in fact, we find negative leakage &#8212; an influx!). We thus conclude that it is plausible the majority of the accuracy is driven by non-robust features, exactly the thesis of  <d-cite key="ilyas2019adversarial"></d-cite>. 
  </p>
<!--   <p>
  The contrasting results, thus, point to methodological advise &#8212; when disentangling two cues in an image, we believe safer is better to drive correlations negative than it is to 0.
  </p>  -->

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    Shan Carter (started the project), Preetum (technical discussion), Chris Olah (technical discussion), Ria (technical discussion), Aditiya (feedback)
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
