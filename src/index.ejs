<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>
<style>
body {
  margin: 40px;
}

li {
margin-bottom:0 !important
}

ul {
margin-bottom:0 !important
}
.wrapper {
  display: grid;
  grid-template-columns: 100px 100px 100px;
  grid-gap: 10px;
  background-color: #fff;
  color: #444;
  width: 50%;
  margin: 0 auto;
}

.box {
  background-color: #444;
  color: #fff;
  border-radius: 5px;
  padding: 20px;
  font-size: 150%;
}


.left {
  text-align: left;
  line-height: 20px
}

.right {
  text-align: left;
  line-height: 20px
}

.grey {
  color:rgb(140,140,140);
}


.mono {
  font-family: monospace;
}

</style>


<d-front-matter>
  <script type="text/json">{
  "title": "Evidence for the presence of robust features in adversarial examples",
  "description": "An example project using webpack and svelte-loader and ejs to inline SVGs",
  "password": "bugs",
  "authors": [
    {
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>

<d-article>

 <p>
A reading of  <d-cite key="ilyas2019adversarial"></d-cite> may lead one to conclude that adversarial examples are chimeric images for which robust cues point towards the original label and non-robust cues point towards the attacked label. We contend, however, that the perturbations in adversarial examples may contain some robust cues. 
</p>

<p>
We are concerned, in particular, with the extent to which robust features may drive the accuracies reported in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite>. We refer to this problem as <i>robust feature leakage</i>. In this manuscript, we attempt to quantify the degree of leakage of robust features in the non-robust datasets generated from adversarial examples. Our technique for lower bounding leakage consisting of two steps:

    <ul>
      <li>First, we construct features $f_i$ that are provably robust, in a sense will soon define.</li>
      <li>Next, we train a linear classifier <d-footnote>as per <d-cite key="ilyas2019adversarial"></d-cite>, Equation 3</d-footnote> on the datasets $\hat{\mathcal{D}}_{\text{det}}$ and $\hat{\mathcal{D}}_{\text{rand}}$ (Defined <d-cite key="ilyas2019adversarial"></d-cite>, Table 1) on these robust features <i>only</i>.
    </ul>
  </p>

  <p>
  We consider accuracy of the linear classifier above as the accuracy explained by robust features only. We propose two definitions for <i>robust feature</i>:

    <ul>
      <li><i>($\gamma$-robust usefulness <d-cite key="ilyas2019adversarial"></d-cite>)</i> A feature that is $\gamma$-robustly useful for at least one of the classes, with $\gamma = 0$, and the set of valid perturbations equal to an $L_2$ norm ball with radius 0.25.</li>
      <li><i>(Robustness Radius)</i> A feature that comes from a robust model for which at least 80% of points in the test set have predictions that remain static in a neighborhood of radius 0.25 on the $L_2$ norm ball.
    </ul>
  </p>

  <p>
  We find features that satisfy <i>both</i> conditions by using the 10 linear features of a robust linear model trained on CIFAR-10. Because the features are linear, the above two conditions can be certified analytically. Finally, we leave the reader to inspect the weights corresponding to the features manually:
  </p>
  <figure id="robust" style="grid-column: screen;">
  </figure>

  <p>
  Training a linear model on the above robust features on $\hat{\mathcal{D}}_{\text{rand}}$ incurs an accuracy of <b>23.5%</b> (out of 88%), while training on $\hat{\mathcal{D}}_{\text{det}}$ incurs an accuracy of <b>6.81%</b> (out of 44%).
  </p>
  <p>
  The contrasting results suggest that the the two experiements should be interpreted differently. The transfer results of $\hat{\mathcal{D}}_{\text{rand}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> should approached with caution: A non-trivial portion of the accuracy can be attributed to robust features. This bound is too, a lower bound based on linear features alone. This number may be higher if we used nonlinear robust features, taken from, say, a robust deep neural network.
  </p>
  <p>
  The results of $\hat{\mathcal{D}}_{\text{rand}}$ in Table 1 of <d-cite key="ilyas2019adversarial"></d-cite> however, are on stronger footing. We find no evidence of feature leakage (in fact, we find negative leakage &#8212; an influx!). We thus conclude that it is very plausible the majority of the accuracy is driven by non-robust features, exactly the thesis of  <d-cite key="ilyas2019adversarial"></d-cite>. 
  </p>
<!--   <p>
  The contrasting results, thus, point to methodological advise &#8212; when disentangling two cues in an image, we believe safer is better to drive correlations negative than it is to 0.
  </p>  -->

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
